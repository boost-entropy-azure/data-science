
== Frequently Asked Questions (FAQ)
Following are the answers to some frequently asked questions.

=== How do I log into the DevOps Docker container
. In a terminal, run `docker run -dit --name <devops-container-name> chesapeaketechnology/devops:latest /bin/bash` to download the latest container image
. Log into the newly downloaded container by running `docker exec -u 0 -it datasci /bin/bash`

=== How do I check the status of Spark jobs
. Log into the Azure portal
. Navigate to the Databricks workspace
. Launch the Databricks namespace and log in
. Select 'Workflows' on the left-hand side eventhubs_mqtt_view_rule_name
. Select the job you're looking for from the list
. Select Logs link under the `Spark` column in the table

=== How do I confirm messages are flowing from my device through the pipeline
. Log into Azure portal
. Look at <pipeline-name>-mqtt-eventhubs-namespace resource page and look at the metrics graph, where
... `<pipeline-name>` is the combination of `cluster_name` and `environment` variables specified during deployment
(see <<Step-by-step Guide, Deployment Section>> for details and `TF_VAR_cluster_name` and `TF_VAR_environment` in particular)
. If messages are not getting to the eventhubs namespace
.. Browse to the <pipeline-name>-mqtt Container Instance resource page
.. Look at the logs of the `mqtt` container. No errors should be shown
.. Look at the logs of the `connector` container. No errors should be shown
.. Make sure your device is able to connect to the MQTT broker and is sending messages
. If messages are flowing to the eventhubs namespace, next check the specific EventHubs instance (topic) that you're
interested in
.. If messages are not getting to the specific EventHub instance, make sure your sensor is actually collecting and sending
the messages you're expecting to see
.. If messages are getting to the EventHub instance, next look at <pipeline-name>lake Storage Account resource page
... Browse to 'Containers/<pipeline-name>-container/<pipeline-name>-mqtt-eventhubs-namespace/<message_topic>'
... Browse to an avro file corresponding to the current date/time. If a recent file exists, the messages are landing
into the data lake
. Look at the <<How do I check the status of Spark jobs, Spark job status page>>
