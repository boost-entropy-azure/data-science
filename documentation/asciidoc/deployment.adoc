== Data Science Infrastructure Deployment

Use this project to deploy an instance of a data science pipeline to Azure using Terraform and Ansible.

Currently, the main Terraform datasci.tf script deploys the following resources to Azure.

- A Resource Group (optional)
- A Virtual Network
- A storage accounts for boot diagnostics
- An Event Hubs Namespace, and an Event Hub instance per specified MQTT topic
- An Azure data lake storage container
- An Azure PostgreSQL server
- An AKS cluster
- A Databricks workspace
- A Redis cache

=== Step-by-step Guide
The data-science project is meant to be deployed from a DevOps Docker container which includes all the needed tools pre-installed and configured. To get started,

. `Install Docker` by following directions http://docs.docker.com/engine/install[here]
. In a terminal, run `docker run -dit --name datasci chesapeaketechnology/devops:latest /bin/bash` to download the container image
. Log into the newly downloaded container by running `docker exec -u 0 -it datasci /bin/bash`
. Set the needed environment variables as shown in the example below.
.. If deploying to a 'blank' subscription, set TF_VAR_manage_* variables to 'true'. If deploying to a prepared environment (such as the MLZ) set those to 'false'
.. Copy the entire code block below and paste it into a text document (VSCode is recommended).
.. Update the values as necessary, specifically the cluster name, default tags, and Azure Account Credentials likely need to be updated.
.. Copy the entire code block out of VSCode and paste it into the terminal of the devops container and hit the "Enter" key.

[source,bash]
----
## Set Pipeline Environment Variables
export WORKING_DIR="/opt/repos"
export TF_VAR_location="eastus" # For Azure US Government: export TF_VAR_location="usgovvirginia"
export TF_VAR_environment="dev"
export TF_VAR_cluster_name="datasci"
export TF_VAR_manage_resource_group="true"
export TF_VAR_resource_group_name="${TF_VAR_cluster_name}-${TF_VAR_environment}-rg"
export TF_VAR_manage_virtual_network="true"
export TF_VAR_virtual_network_name="${TF_VAR_cluster_name}-${TF_VAR_environment}-vnet"
export TF_VAR_manage_subnet="true"
export TF_VAR_subnet_name="${TF_VAR_cluster_name}-${TF_VAR_environment}-snet"

export TF_VAR_tfstate_resource_group_name="rg-${TF_VAR_cluster_name}-tfstate-${TF_VAR_environment}"

export TF_VAR_flux_repo_url="https://gitlab.ctic-dev.com/engineering/dfp/pipeline/flux.git"
export TF_VAR_flux_repo_branch="master"
export TF_VAR_flux_target_path="./clusters/staging"
export TF_VAR_flux_harbor_registry="https://harbor.eng.ctic-dev.com"

# GitLab Token is used to authenticate when FluxCD is pulling a repo from our company GitLab instance
export TF_VAR_gitlab_token="YOUR_GITLAB_TOKEN"

export TF_VAR_state_container="remote-tfstates"
export TF_VAR_default_tags=$(printf '{"Department"="Engineering","PoC"="Me","Environment"="%s","IaC_Managed"="Yes"}' $(echo ${TF_VAR_environment^^}))
export TF_VAR_alert_topics='["alert_message"]'
export TF_VAR_mqtt_topics='["gsm_message","cdma_message","umts_message","lte_message","nr_message","80211_beacon_message","bluetooth_message","gnss_message","device_status_message","cellular_ota_message"]'
export TF_VAR_jobs='["gnss-monitor","cgi-monitor","phone-state-monitor","gnss-cache-monitor","cgi-alert-monitor"]'

# Azure Account Credentials
export ARM_ENVIRONMENT="public"
export ARM_CLIENT_ID="azure-serviceprincipal-client-id" # Called Application ID in Azure
export ARM_CLIENT_SECRET="azure-serviceprincipal-secret"
export ARM_SUBSCRIPTION_ID="azure-subscription-id"
export ARM_TENANT_ID="azure-tenant-id"
# k8s subscription id
export TF_VAR_k8s_subscription_id="azure-subscription-id" # if no separate k8s subscription is used, set this to TF_VAR_k8s_subscription_id="${$ARM_SUBSCRIPTION_ID}"

# Remote State Configuration
export TF_VAR_remotestate_client_id="${ARM_CLIENT_ID}"
export TF_VAR_remotestate_client_secret="${ARM_CLIENT_SECRET}"
export TF_VAR_remotestate_subscription_id="${ARM_SUBSCRIPTION_ID}"
export TF_VAR_remotestate_tenant_id="${ARM_TENANT_ID}"
export TF_VAR_remotestate_storage_account_name="${TF_VAR_environment}tfstate${TF_VAR_cluster_name}001"
export TF_VAR_sp_password="${ARM_CLIENT_SECRET}"
----

[start=5]
. Clone the data-science repo by running:
[source,bash]
----
mkdir -p ${WORKING_DIR}
cd ${WORKING_DIR}
git clone https://github.com/chesapeaketechnology/data-science.git
cd data-science/
git checkout master
echo $(pwd)
echo "We should be ready to fire away."
----

[start=6]
. *First Time Only*: This step should only be done if this is the first time you're deploying the data-science repo to your environment, or if you're re-deploying to an existing environment following a complete destruction of the environment (i.e. the Terraform remote state storage account does not have any state information on the environment you're about to deploy).

[source,bash]
----
cd ${WORKING_DIR}/data-science/terraform/remote-state-bootstrap
terraform init
terraform import azurerm_resource_group.resource_group /subscriptions/${ARM_SUBSCRIPTION_ID}/resourceGroups/${TF_VAR_tfstate_resource_group_name}
terraform apply -auto-approve
# OCD optional cleanup (is ignored by Git via .gitignore)
rm -fr ./.terraform *.tfstate*
----

[start=7]
. Log into Azure CLI

[source,bash]
----
az account clear
az login --service-principal --username ${ARM_CLIENT_ID} --tenant ${ARM_TENANT_ID} --password ${ARM_CLIENT_SECRET}
----

[start=8]
. Deploy the *infrastructure* resources of the project

NOTE: The first `terraform apply` command often fails with an error indicating the resource group does not exist. If this happens run the command again since Azure has likely finished creating the resource group by the time you re-run the command.

[source,bash]
----
cd ${WORKING_DIR}/data-science/terraform/providers/infrastructure
terraform init \
-backend-config="key=${TF_VAR_environment}/$(basename $(pwd)).tfstate" \
-backend-config="resource_group_name=${TF_VAR_tfstate_resource_group_name}" \
-backend-config="storage_account_name=${TF_VAR_remotestate_storage_account_name}" \
-backend-config="container_name=${TF_VAR_state_container}" \
-backend-config="client_id=${TF_VAR_remotestate_client_id}" \
-backend-config="client_secret=${TF_VAR_remotestate_client_secret}" \
-backend-config="subscription_id=${TF_VAR_remotestate_subscription_id}" \
-backend-config="tenant_id=${TF_VAR_remotestate_tenant_id}"
terraform apply
----

[start=9]
. Deploy the *application* resources of the project

[source,bash]
----
cd ${WORKING_DIR}/data-science/terraform/providers/application_stack
terraform init \
-backend-config="key=${TF_VAR_environment}/$(basename $(pwd)).tfstate" \
-backend-config="resource_group_name=${TF_VAR_tfstate_resource_group_name}" \
-backend-config="storage_account_name=${TF_VAR_remotestate_storage_account_name}" \
-backend-config="container_name=${TF_VAR_state_container}" \
-backend-config="client_id=${TF_VAR_remotestate_client_id}" \
-backend-config="client_secret=${TF_VAR_remotestate_client_secret}" \
-backend-config="subscription_id=${TF_VAR_remotestate_subscription_id}" \
-backend-config="tenant_id=${TF_VAR_remotestate_tenant_id}"
terraform apply
----

[start=10]
. Optional - Deploy the *analysis* resources of the project

The Analysis jobs have not been made public yet. Reach out to CTI to get access to them.

[source,bash]
----
cd ${WORKING_DIR}/data-science/terraform/providers/analysis
terraform init \
-backend-config="resource_group_name=${TF_VAR_tfstate_resource_group_name}" \
-backend-config="storage_account_name=${TF_VAR_remotestate_storage_account_name}" \
-backend-config="container_name=${TF_VAR_state_container}" \
-backend-config="client_id=${TF_VAR_remotestate_client_id}" \
-backend-config="client_secret=${TF_VAR_remotestate_client_secret}" \
-backend-config="subscription_id=${TF_VAR_remotestate_subscription_id}" \
-backend-config="tenant_id=${TF_VAR_remotestate_tenant_id}"
cd .terraform/modules/analysis_jobs/
export GITLAB_PRIVATE_TOKEN=<token-value>
./gradlew getJobArtifacts
cd ${WORKING_DIR}/data-science/terraform/providers/analysis
terraform apply
----

=== Viewing the Terraform Output

- Once deployed, the outputs below will assist in accessing or managing the environment:
[source,bash]
----
terraform -chdir=${WORKING_DIR}/data-science/terraform/providers/infrastructure output -json | jq -r '.automation_account_ssh_private.value'
terraform -chdir=${WORKING_DIR}/data-science/terraform/providers/application_stack output -json | jq -r '.datasci_node_public_ips.value'
terraform -chdir=${WORKING_DIR}/data-science/terraform/providers/application_stack output -json | jq -r '.grafana_admin_password.value.result'
----

- The following outputs are needed to pass into Kubernetes for the application portion of the data science pipeline
- The following command prints out all the outputs from the last Terraform run stored in the tfstates file for the application stack.
[source,bash]
----
terraform -chdir=${WORKING_DIR}/data-science/terraform/providers/application_stack output -json | jq -r
----

- The same as above but for the infrastructure.
[source,bash]
----
terraform -chdir=${WORKING_DIR}/data-science/terraform/providers/infrastructure output -json | jq -r
----

- If you want to print out a specific item from the Terraform run outputs, then use something like:
[source,bash]
----
terraform -chdir=${WORKING_DIR}/data-science/terraform/providers/application_stack output -json | jq -r '.eventhubs_mqtt_namespace_fqn.value'
terraform -chdir=${WORKING_DIR}/data-science/terraform/providers/application_stack output -json | jq -r '.eventhubs_mqtt_namespace_connection_string.value'
terraform -chdir=${WORKING_DIR}/data-science/terraform/providers/application_stack output -json | jq -r '.eventhubs_mqtt_view_primary_key.value'
terraform -chdir=${WORKING_DIR}/data-science/terraform/providers/application_stack output -json | jq -r '.eventhubs_mqtt_view_rule_name.value'
----

=== Destruction

Destruction of assets will not be an automated process. Tread with caution, as this is permanent and **WILL** result in
data loss.

To remove an environment:

1. Analysis Jobs: `terraform -chdir=${WORKING_DIR}/data-science/terraform/providers/analysis destroy`
2. Application Stack: `terraform -chdir=${WORKING_DIR}/data-science/terraform/providers/application_stack destroy`
3. Infrastructure: `terraform -chdir=${WORKING_DIR}/data-science/terraform/providers/infrastructure destroy`

NOTE: Based on the Terraform bootstrap process, running the destroy command **WILL NOT** remove the Terraform state data or storage container, as that is (and should be) provisioned outside the main infrastructure states to ensure environment safety.


=== Troubleshooting
* After running a full `terraform destroy` and Azure still shows 2 resources (Network security group and Virtual Network), execute the following:
** This assumes you are logged into the container per the Deployment Process

[source,bash]
----
az network profile delete --id $(az network profile list | jq -r '.[].id') -y
az network vnet delete --resource-group $(az network vnet list | jq -r '.[].resourceGroup') --name $(az network vnet list | jq -r '.[].name')
az network nsg delete --resource-group $(az network nsg list | jq -r '.[].resourceGroup') --name $(az network nsg list | jq -r '.[].name')
----


* If you get an error about _"The subscription is not registered to use namespace Microsoft.Network"_, then use the steps on the following page to register the *Microsoft.Network* resource provider.
** https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/error-register-resource-provider
** Something like:
*** `az provider register --namespace Microsoft.Network`