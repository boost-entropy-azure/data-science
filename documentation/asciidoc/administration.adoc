
== Administering the Data Science Pipeline

Data Science Pipeline Overview

The core purpose of this project is to create a cloud-based framework enabling the analysis of sensor data. A high-level
diagram of the pipeline components is shown below.

image::DataSciencePipeline.png[Pipeline Diagram]

=== Sensors
These devices are present and sensing the world around them. The data they collect contains useful information, and extracting
that information is what this pipeline enables. Note that the data-centric analysis jobs needed to extract information out of
sensor data is not in the scope of this project. Instead, all we provide are the building blocks we found useful for extracting
information out of the massive sensor data we collect.

Integration of devices is done through a device gateway. The primary device gateway used by this pipeline is an
MQTT broker. The specific messages sent from the device into the MQTT broker, and then processed by the backend
consumers is up to the implementation of this pipeline, but it is recommended to use a tool such as AsyncAPI to document
the message schema. For example, the https://messaging.networksurvey.app/[Network Survey Messaging API definition] can
be used with this pipeline. Devices send JSON messages to the MQTT broker that follow the Network Survey Messaging
schema, and then any consumers can also follow the schema to process the JSON messages.

=== Reverse Proxy (Istio Gateway)
This is the gateway for all the sensors to authenticate against and send their messages to. The reverse proxy accepts
connections from devices and passes them onto a Mosquitto MQTT broker.

Additionally, the Istio gateway also allows for https access to our DFP Reveal web interface as well as the Grafana dashboards. 
The default Grafana dashboards present a simple overview of the status of the pipeline and the data flowing through it. Using 
the default dashboards as a starting point for building more custom ones is the best way to gain further insight into the data 
flowing through.

=== AKS cluster
The AKS cluster consists of several pods.

***MQTT pods
. The `pipeline-mqtt-broker` pod has two conatiners
.. The `mqtt-broker` container is responsible for authenticating the users sending messages into the pipeline and
enqueuing incoming messages onto their respective Mosquitto MQTT topics.
.. The `mqtt-broker-exporter` container is responsible for exporting Mosquitto MQTT broker metrics in the Prometheus format.
. The `pipeline-mqtt-connector` pod is responsible for reading all incoming topics from the MQTT broker and posting the messages to the equivalent
eventhub topics
. The `pipeline-connector-*` pods are responsible for subscribing to configured MQTT topics and passing messages onto the
corresponding EventHubs instances.

***Consul pods
. The `consul-server-*` pods are responsible for storing credentials so that different services are able to authenticate amongst each other

***Reveal UI pods
. The `reveal-ui` is responsible for the DFP reveal UI webpage
. The `pipeline-alerts`, `pipeline-surveys`, `pipeline-device-statuses`, `pipeline-brokersettings` are responsible for providing and API
to the data within the pipeline so that different widgets, pages and features of the DFP Reveal work

***Grafana pods
. The `pipeline-grafana` pod is responsible for running an instance of Grafana and displaying the provisioned dashboards

***Loki pods
. The `pipeline-loki-*` pods are responsible for aggregating logs
. The `pipeline-promtail` pods are responsible for delivering the aggregated logs to Grafana

***Prometheus pods
. The `pipeline-prometheus-server` pod is responsible for running a Prometheus server
. The `pipeline-prometheus-*-exporter` pods are responsible for sending telemetry data from nodes, Consul, etc. to the Proetheus server

=== MQTT Event Hubs Namespace
This namespace includes the provisioned Event Hubs instances matching the number of MQTT topics. Each instance is
configured to forward its messages to the provisioned data lake every 5 minutes or 300 MB which ever occurs first. If no
data is received on the instance, an empty data file will not be created in the data lake.

=== Data Lake
This resource uses the Data Lake Storage Gen2 Azure blob storage with hierarchical namespaces enabled.

=== Databricks cluster
The analytics Databricks cluster is responsible for providing a mechanism for processing the data streams flowing through the pipeline. 

=== Alerts Event Hubs Namespace
This set of Event Hubs is responsible for storing any alerts generated by the pipeline monitoring system, the analytic
data processing jobs, etc.
