
== Administering the Data Science Pipeline

Data Science Pipeline Overview

The core purpose of this project is to create a cloud-based framework enabling the analysis of sensor data. A high-level
diagram of the pipeline components is shown below.

image::DataSciencePipeline.png[Pipeline Diagram]

=== Sensors
These devices are present and sensing the world around them. The data they collect contains useful information, and extracting
that information is what this pipeline enables. Note that the data-centric analysis jobs needed to extract information out of
sensor data is not in the scope of this project. Instead, all we provide are the building blocks we found useful for extracting
information out of the massive sensor data we collect.

Integration of devices is done through a device gateway. The primary device gateway used by this pipeline is an
MQTT broker. The specific messages sent from the device into the MQTT broker, and then processed by the backend
consumers is up to the implementation of this pipeline, but it is recommended to use a tool such as AsyncAPI to document
the message schema. For example, the https://messaging.networksurvey.app/[Network Survey Messaging API definition] can
be used with this pipeline. Devices send JSON messages that follow the Network Survey Messaging schema to the MQTT
broker, and then any consumers can also follow the schema to process the JSON messages.

=== Reverse Proxy (NGINX Gateway)
This is the gateway for all the sensors to authenticate against and send their messages to. The reverse proxy accepts
connections from devices and passes them onto a Mosquitto MQTT broker.

Additionally, the NGINX gateway also allows for https access to our Grafana instance. The default Grafana dashboards present
a simple overview of the status of the pipeline and the data flowing through it. Using the default dashboards as a starting
point for building more custom ones is the best way to gain further insight into the data flowing through.

=== MQTT cluster
This Azure container instance consists of four containers.

. The `mqtt` container is responsible for authenticating the users sending messages into the pipeline and
enqueuing incoming messages onto their respective Mosquitto MQTT topics.
. The `connector` container is responsible for subscribing to configured MQTT topics and passing messages onto the
corresponding EventHubs instances.
. The `mqttconsulgateway` is responsible for starting up a Consul client instance and providing insight into provisioned
MQTT user credentials.
. The `mqttexporter` is responsible for exporting Mosquitto MQTT broker metrics in the Prometheus format.

=== MQTT Event Hubs Namespace
This namespace includes the provisioned Event Hubs instances matching the number of MQTT topics. Each instance is
configured to forward its messages to the provisioned data lake every 5 minutes or 300 MB which ever occurs first. If no
data is received on the instance, an empty data file will not be created in the data lake.

=== Data Lake
This resource uses the Data Lake Storage Gen2 Azure blob storage with hierarchical namespaces enabled.

=== Stream Processing cluster
This cluster of CentOS VMs is responsible for:

. Providing a mechanism for processing the data streams flowing through the pipeline. To this end, we deploy Apache
Spark on all the nodes as well as YARN for workload management. The default number of VMs provisioned is 3. To adjust
this setting, look at `node_count` variable in the Terraform script datasci_vars.tfvars
. Setting up the Consul server nodes. We primarily use the Consul Key/Value store for storing the deployment facts and
secrets in order to simplify integrating components as well credential look up post deployment.

=== Alerts Event Hubs Namespace
This set of Event Hubs is responsible for storing any alerts generated by the pipeline monitoring system, the analytic
data processing jobs, etc.

=== Grafana server cluster
This container instance named <pipeline-name>-grafana consists of a single `grafana-server` container. This container
is responsible for setting up the Grafana server

=== Grafana BE cluster
This container instance named <pipeline-name>-grafana-be-topic-integration consists of a set of containers responsible
for consuming messages from the EventHubs instances in the <<Alerts Event Hubs Namespace>>

=== Grafana FE cluster
This container instance named <pipeline-name>-grafana-fe-topic-integration consists of a set of containers responsible
for consuming messages from the EventHubs instances in the <<MQTT Event Hubs Namespace>. All the consumed messages
are indexed and stored in the dbs<pipeline-name>-data Azure Database for PostgreSQL resource.

=== Prometheus cluster
This container instance named <pipeline-name>-monitor consists of two containers.

. `prometheus` container is responsible for setting up a Prometheus server
. `consul-exporter` container is responsible for exporting Consul metrics in Prometheus format

[NOTE]
====
`<pipeline-name>` is the combination of `cluster_name` and `environment` variables specified during deployment
(see {uri-org}{doc-path}/deployment/docs/DeploymentManual.adoc[DeploymentManual] for details and datasci_vars.tfvars in particular)
====
